{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7e77cf",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "## for covid-19 cases and deaths + hospital capacity data\n",
    "first let's confirm the correct libraries are installed. For more information on how to set-up your Jupyter Notebook check out the `README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d822b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def install_if_not_exists(module_names):\n",
    "    for module_name in module_names:\n",
    "      try:\n",
    "          importlib.import_module(module_name)\n",
    "          print(f\"{module_name} is already installed.\")\n",
    "      except ImportError:\n",
    "          %pip install {module_name}\n",
    "          print(f\"{module_name} has been installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e1392",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_modules = [\"pandas\", \"matplotlib\", \"seaborn\", \"numpy\", \"scipy\", \"IPython\"]\n",
    "install_if_not_exists(required_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303df820-31e8-470e-b70e-1b79618eed97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5a9fa-e2b4-4e6a-98a5-f7a61e279aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable inline plotting in Jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8a3e4-9c13-422d-909e-8319427c5753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_deaths = \"data/Weekly_United_States_COVID-19_Cases_and_Deaths_by_County_-_ARCHIVED_20240113.csv\"\n",
    "data_hosp = \"data/COVID-19_Reported_Patient_Impact_and_Hospital_Capacity_by_Facility_20240114.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1aa297-e745-44c2-b777-f61334c6b579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_d = pd.read_csv(data_deaths)\n",
    "df_h = pd.read_csv(data_hosp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec569d-3545-449c-a755-f31dbb745530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c29fe-0d13-4603-99ad-841193329be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248153b7-5819-4e24-bfe5-cc514eb1fd71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(df_d.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716fc56-24e2-460b-8b14-5ecbe1caba2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(df_h.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c797b-584a-4ce6-95d5-e8c326846e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# might be good for later mapping data\n",
    "df_h['geocoded_hospital_address'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df44dc9-3d52-48d2-ac88-92c80e9136c9",
   "metadata": {},
   "source": [
    "## Dataset Clean-Up Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726eaa91-b510-4a5f-9bb4-f25ba4c2ad29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with each entry as columns_h[i]: 'sum'\n",
    "def get_agg_dict(columns:list) -> dict:\n",
    "    return {col: 'sum' for col in columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5a87f-81d7-42d3-9fb4-d3ee0ad02795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_merged_data(weekly_aggregated_cases_STATE:pd.core.frame.DataFrame, weekly_aggregated_hospitals_STATE:pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    merged_df = pd.merge(weekly_aggregated_cases_STATE, weekly_aggregated_hospitals_STATE, on=['year', 'week', 'is_metro_micro'])\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['year'].astype(str) + merged_df['week'].astype(str) + '1', format='%Y%U%w')\n",
    "    merged_df = merged_df.drop(columns=['year', 'week']).set_index('date')\n",
    "\n",
    "    # Set the threshold count (52 in this case aka at least 1-years worth of data)\n",
    "    threshold_count = 52\n",
    "\n",
    "    # Filter columns based on count\n",
    "    selected_columns = merged_df.columns[merged_df.count() >= threshold_count]\n",
    "\n",
    "    # Create a new DataFrame with selected columns\n",
    "    merged_df = merged_df[selected_columns]\n",
    "    \n",
    "    print(\"The shape of the merged_df is\", merged_df.shape[0], \"by\", merged_df.shape[1])\n",
    "    assert sum(merged_df['is_metro_micro'].unique() == 1), \"The column 'is_metro_micro' should only have 0 or 1 as values\"\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938b931-cbf8-4f7f-96c5-a05723068e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_sub_state_cleaned(df:pd.core.frame.DataFrame, \n",
    "                             columns_to_filter:list, \n",
    "                             date_col:str='collection_week',\n",
    "                             state:str='MI',\n",
    "                             merge_county_data_enabled:bool=False, \n",
    "                             county_df:pd.core.frame.DataFrame=None) -> pd.core.frame.DataFrame:\n",
    "    \n",
    "    if (date_col == 'collection_week'):\n",
    "        columns_for_subset = ['collection_week', 'state', 'hospital_name', 'is_metro_micro', 'geocoded_hospital_address'] + columns_to_filter\n",
    "    else:\n",
    "        columns_for_subset = ['fips_code', 'state', 'date'] + columns_to_filter\n",
    "\n",
    "    df_sub = df[columns_for_subset]\n",
    "    df_sub[date_col] = pd.to_datetime(df_sub[date_col])\n",
    "    df_sub['year'] = df_sub[date_col].dt.year\n",
    "    df_sub['week'] = df_sub[date_col].dt.isocalendar().week\n",
    "    \n",
    "    if (df_sub[columns_to_filter].isnull().any().any()):\n",
    "        # Replace NaN values with 0\n",
    "        print('NaN values present and being replaced with zeros')\n",
    "        df_sub[columns_to_filter] = df_sub[columns_to_filter].fillna(0)\n",
    "        \n",
    "    # Replace negative values with 0 (negative values are insertion errors)\n",
    "    df_sub[columns_to_filter] = df_sub[columns_to_filter].applymap(lambda x: max(0, x))\n",
    "    \n",
    "    # exploring hospital data per STATE (default MI aka Michigan)\n",
    "    df_STATE = df_sub[df_sub['state'] == state]\n",
    "    \n",
    "    if merge_county_data_enabled:\n",
    "        # Check if county_df is None\n",
    "        assert county_df is not None, \"Input DataFrame 'county_df' cannot be None\"\n",
    "        df_STATE = pd.merge(df_STATE, county_df, on='fips_code')\n",
    "\n",
    "    df_STATE['is_metro_micro'] = df_STATE['is_metro_micro'].astype(int)\n",
    "    \n",
    "    return df_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e59770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_data_per_state(states:list):\n",
    "    current_directory = str(os.getcwd()).replace('\\\\','/')\n",
    "    print(f\"saving files to {current_directory}\")\n",
    "\n",
    "    for state in states:\n",
    "        folder_path = f\"/data/outputs/{state}\"\n",
    "        file_name_h = f\"/df_h_sub_cleaned_{state}.csv\"\n",
    "        file_name_d = f\"/df_d_sub_cleaned_{state}.csv\"\n",
    "        output_folder_path = current_directory + folder_path\n",
    "\n",
    "        df_h_sub_cleaned = get_df_sub_state_cleaned(df_h, columns_h, date_col='collection_week', state=state)\n",
    "        df_d_sub_cleaned = get_df_sub_state_cleaned(df_d, columns_d, date_col='date', state=state, merge_county_data_enabled=True, county_df=county_df)\n",
    "\n",
    "        # Check if the folder exists; if not, create it\n",
    "        if not os.path.exists(output_folder_path):\n",
    "            os.makedirs(output_folder_path)\n",
    "            print(output_folder_path)\n",
    "\n",
    "        print(f\"saving data for {state} ...\")\n",
    "        df_h_sub_cleaned.to_csv((output_folder_path + file_name_h), index=False)\n",
    "        df_d_sub_cleaned.to_csv((output_folder_path + file_name_d), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0a658-bd93-498c-905a-e1bad5baacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekly_aggregated_data(df_sub_STATE:pd.core.frame.DataFrame, columns:list, replace_zeros:bool=False) -> pd.core.frame.DataFrame:  \n",
    "\n",
    "    weekly_aggregated_STATE = df_sub_STATE.groupby(['year', 'week', 'is_metro_micro']).agg(get_agg_dict(columns)).reset_index()\n",
    "    \n",
    "    if replace_zeros:\n",
    "        # Replace all 0 values back with NaN so that they do not affect the correlation data\n",
    "        weekly_aggregated_STATE[columns] = weekly_aggregated_STATE[columns].replace(0, np.nan)\n",
    "        \n",
    "    assert sum(weekly_aggregated_STATE['is_metro_micro'].unique() == 1), \"The column 'is_metro_micro' should only have 0 or 1 as values\"\n",
    "    \n",
    "    return weekly_aggregated_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87d486-8b79-4ee6-8415-ec94cae71dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_columns(merged_df:pd.core.frame.DataFrame, columns_to_correlate:list) -> tuple:\n",
    "    \"\"\"\n",
    "    If you're observing NaN values in the correlation matrix heatmap, it might be due to a situation where the standard deviation of one of the columns involved in the correlation calculation is zero. This can happen when the data in one of the columns is constant across all rows.\n",
    "    In a correlation calculation, when the standard deviation is zero, the denominator in the correlation formula becomes zero, leading to division by zero and resulting in NaN values.\n",
    "    To handle this situation, you can preprocess your data to handle constant columns or columns with zero variance.\n",
    "    \"\"\"\n",
    "    \n",
    "    high_correlation_arr = []\n",
    "\n",
    "    for category in range(2):\n",
    "\n",
    "        metro_micro_df =  merged_df[merged_df['is_metro_micro'] == category]\n",
    "\n",
    "        # Identify and drop constant columns\n",
    "        constant_columns = metro_micro_df.columns[metro_micro_df.nunique() == 1]\n",
    "        metro_micro_df = metro_micro_df.drop(columns=constant_columns)\n",
    "\n",
    "        # Select the specified columns along with all other columns in the DataFrame\n",
    "        selected_columns = columns_to_correlate + metro_micro_df.columns.difference(columns_to_correlate).tolist()\n",
    "        \n",
    "        # Calculate the correlation matrix for the selected columns\n",
    "        correlation_matrix = metro_micro_df[selected_columns].corr() # method='spearman'\n",
    "        correlation_matrix = correlation_matrix.dropna(how='all')\n",
    "        correlation_matrix = correlation_matrix.dropna(axis=1, how='all')\n",
    "        \n",
    "        # display correlation heat map\n",
    "        show_heat_map(correlation_matrix.loc[columns_to_correlate], category)\n",
    "        \n",
    "        # get high correlation columns\n",
    "        high_correlation_columns = (correlation_matrix.loc[columns_to_correlate].columns[((correlation_matrix.loc[columns_to_correlate].abs() > 0.7) & \n",
    "                                   (correlation_matrix.loc[columns_to_correlate].abs() < 1)).any()].tolist())\n",
    "        \n",
    "        high_correlation_columns = [col for col in high_correlation_columns if col not in columns_to_correlate]\n",
    "        \n",
    "        # check that the column has a reasonable amount of data for that category\n",
    "        high_correlation_columns = [col for col in high_correlation_columns if (metro_micro_df[col].dropna().shape[0] > 10)]\n",
    "        \n",
    "        high_correlation_arr.append(high_correlation_columns)\n",
    "        \n",
    "    return high_correlation_arr[0], high_correlation_arr[1], correlation_matrix.loc[columns_to_correlate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4a366-2f52-403a-b0fb-9548d24e308a",
   "metadata": {},
   "source": [
    "## Data Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241e6a1-c2b2-4ac8-8c03-def0e94028b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heat_map(correlation_matrix:pd.core.frame.DataFrame, category:int=1):\n",
    "    \n",
    "    if (category == 1):\n",
    "        title_text = \"Urban\"\n",
    "    else:\n",
    "        title_text = \"Rural\"\n",
    "    \n",
    "    # Plot the correlation heatmap\n",
    "    plt.figure(figsize=(30, 3))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, annot_kws={'rotation': 90})\n",
    "    plt.title(f'Correlation Heatmap In {title_text} Areas')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3e23f-d4c7-46f1-b9f8-1144f117b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For high_correlation_columns_urban/rural let's check out their distributions\n",
    "def get_norm_dist_of_high_corr(merged_df:pd.core.frame.DataFrame, \n",
    "                               corr_matrix_sub:pd.core.frame.DataFrame, \n",
    "                               high_correlation_arr:list, \n",
    "                               is_metro_micro:bool):\n",
    "    \n",
    "    category = 'is_metro_micro'\n",
    "    \n",
    "    for col in high_correlation_arr:\n",
    "        sample_data = merged_df[merged_df[category] == is_metro_micro][col]\n",
    "        corr_val = corr_matrix_sub[col][is_metro_micro]\n",
    "\n",
    "        if sample_data.isnull().any():\n",
    "            sample_data.fillna(sample_data.mean(), inplace=True)\n",
    "\n",
    "        df_z_score = zscore(sample_data)\n",
    "\n",
    "        sns.histplot(df_z_score, kde=True, stat='density', label=col + '- Standardized Data')\n",
    "        plt.title('Standard Normal Distribution Curve for \\n' + col + '\\n is_urban = ' + str(is_metro_micro))\n",
    "        plt.xlabel(f'For a given correlation of {round(corr_val,2)}')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ec275-abe9-433b-ab64-d1866b853f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the columns for the scatter plot\n",
    "def get_corr_plot(merged_df:pd.core.frame.DataFrame, \n",
    "                  corr_matrix_sub:pd.core.frame.DataFrame, \n",
    "                  high_correlation_arr:list, \n",
    "                  columns_d:list, \n",
    "                  is_metro_micro:bool):\n",
    "    \n",
    "    category = 'is_metro_micro'\n",
    "    sample_data = merged_df[merged_df[category] == is_metro_micro]    \n",
    "    \n",
    "    for col in high_correlation_arr:\n",
    "        y_column = col\n",
    "        corr_val = corr_matrix_sub[col][is_metro_micro]\n",
    "\n",
    "        # Plot the scatter plot with different colors for each category\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        for i, x_col in enumerate(columns_d):\n",
    "            plt.subplot(1, 2, i+1)\n",
    "            x_column='new_deaths'\n",
    "            sns.scatterplot(data=sample_data, x=x_col, y=y_column, hue=category, palette={0: 'blue', 1: 'orange'}, s=100)\n",
    "            plt.title(f'{y_column} vs {x_col}')\n",
    "            plt.xlabel(f'{x_col} given a {round(corr_val,2)} correlation')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27acca24-ef4d-495c-a05c-39d09f43153d",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa56fb-ee8b-4f46-a56b-e7e4bf88059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df_d:pd.core.frame.DataFrame, df_h:pd.core.frame.DataFrame, county_df:pd.core.frame.DataFrame, columns_d:list, columns_h:list, state:str='MI'):\n",
    "    \n",
    "    print(f\"\\ncreating sub hospital dataframe for {state}\")\n",
    "    df_h_sub = get_df_sub_state_cleaned(df_h, columns_h, date_col='collection_week', state=state)\n",
    "    # display(df_h_sub.sample(5))\n",
    "    print(\"\\nsub dataframe complete\")\n",
    "    \n",
    "    print(f\"\\ncreating sub covid cases/deaths dataframe for {state}\")\n",
    "    df_d_sub = get_df_sub_state_cleaned(df_d, columns_d, date_col='date', state=state, merge_county_data_enabled=True, county_df=county_df)\n",
    "    # display(df_d_sub.sample(5))\n",
    "    print(\"\\nsub dataframe complete\")\n",
    "    \n",
    "    print(\"\\naggregating hospital data\")\n",
    "    weekly_aggregated_hospitals = get_weekly_aggregated_data(df_h_sub, columns_h, replace_zeros=True)\n",
    "    display(weekly_aggregated_hospitals.sample(5))\n",
    "    \n",
    "    print(\"\\naggregating covid cases/deaths data\")\n",
    "    weekly_aggregated_cases = get_weekly_aggregated_data(df_d_sub, columns_d)\n",
    "    display(weekly_aggregated_cases.sample(5))\n",
    "    \n",
    "    print(\"\\nmerging dataframes on week, year, and is_metro_micro\")\n",
    "    merged_df = get_merged_data(weekly_aggregated_cases_STATE=weekly_aggregated_cases, weekly_aggregated_hospitals_STATE=weekly_aggregated_hospitals)\n",
    "    display(merged_df.head())\n",
    "    \n",
    "    print(\"\\nfinding high correlating columns to covid cases and deaths\")\n",
    "    # Get columns with correlation greater than 0.7 and less than 1 for rural areas\n",
    "    high_correlation_columns_rural, high_correlation_columns_urban, corr_matrix_sub = get_correlation_columns(merged_df, columns_d)\n",
    "    \n",
    "    print(\"\\nAll highly correlated columns for RURAL AREAs:\")\n",
    "    print(\"---\")\n",
    "    for col in high_correlation_columns_rural:\n",
    "        print(col, \"has\", merged_df[merged_df['is_metro_micro'] == 0][col].dropna().shape[0], \"data points\")\n",
    "    \n",
    "    print(\"\\nAll highly correlated columns for URBAN AREAs:\")\n",
    "    print(\"---\")\n",
    "    for col in high_correlation_columns_urban:\n",
    "        print(col, \"has\", merged_df[merged_df['is_metro_micro'] == 1][col].dropna().shape[0], \"data points\")\n",
    "    \n",
    "    print(\"\\ndisplaying normal distribution of highly correlated columns\")\n",
    "    get_norm_dist_of_high_corr(merged_df, corr_matrix_sub, high_correlation_columns_rural, 0)\n",
    "    get_norm_dist_of_high_corr(merged_df, corr_matrix_sub, high_correlation_columns_urban, 1)\n",
    "    \n",
    "    print(\"\\ndisplaying correlation plots of highly correlated columns\")\n",
    "    get_corr_plot(merged_df, corr_matrix_sub, high_correlation_columns_rural, columns_d, is_metro_micro=0)\n",
    "    get_corr_plot(merged_df, corr_matrix_sub, high_correlation_columns_urban, columns_d, is_metro_micro=1)\n",
    "    \n",
    "    return merged_df, high_correlation_columns_rural, high_correlation_columns_urban\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8567f-4e1a-4611-be62-1b3c3a0f9d48",
   "metadata": {},
   "source": [
    "## Hospital Dataset Clean-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f2a11-309d-4abf-a0be-e0f0de9635bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's clean up the hospital data columns for only the ones we want to aggregate\n",
    "columns_h = list(df_h.columns)[11:]\n",
    "\n",
    "# remove any column names with influenza or other non-relavant text\n",
    "substrings_to_exclude = ['influenza', 'hhs_ids', 'is_corrected', 'total_personnel_covid_vaccinated', 'geocoded_hospital_address']\n",
    "columns_h = [col for col in columns_h if not any(substring in col for substring in substrings_to_exclude)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd0ebb-3ef0-433e-b29b-52be69039bde",
   "metadata": {},
   "source": [
    "## COVID-19 Cases & Deaths Dataset Clean-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa711f1-41a2-4d87-b97a-4037fa1f87bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first, rename column names so that they match all other column name formats\n",
    "column_mapping_d = {'New cases': 'new_cases', 'New deaths': 'new_deaths'}\n",
    "df_d.rename(columns=column_mapping_d, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac590f-e71b-4389-be1a-990bb382a1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# covid death data\n",
    "df_d.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1875d-fee9-4878-b874-d1b39a900592",
   "metadata": {
    "tags": []
   },
   "source": [
    "### merging in urban vs rural data\n",
    "You may notice that we have an issue here compared to our other dataset. The hospital dataset has information on 'is_metro_micro', however this dataset on covid deaths and cases does not have this differentiation. Therefore we would not be able to combine datasets without losing some of that information, and we cannot assume how the cases and deaths are split up by rural vs urban in this dataset. Therefore we need to extract some more information from the hospital dataset first to be able to combine it with our current dataset. \n",
    "\n",
    "The easiest way to do this, is both datasets have location information based on the column 'fips_code'. We will merge the 'is_metro_micro' data onto our deaths/cases dataset on the 'fips_code' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979b667-62ba-460e-ae89-5a957f5d644b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "county_df = df_h[['fips_code', 'is_metro_micro']].dropna()\n",
    "# county_df = county_df.astype(int)\n",
    "county_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f91c1a-2b54-44cf-b854-2cb622394106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's grab the covid data columns for only the ones we want to aggregate\n",
    "columns_d = ['new_cases', 'new_deaths']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338b62d",
   "metadata": {},
   "source": [
    "Let's save some of the data to our output for future manipulation. Uncomment to run, should take around ~20min to go through all 50 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_cleaned_data_per_state(list(df_d['state'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19706b-67c8-4844-bfef-615420ec4a9b",
   "metadata": {},
   "source": [
    "## Display Correlation Data Per State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0589ef-d166-4332-918d-f719323feebc",
   "metadata": {},
   "source": [
    "### Michigan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c70087-b368-4d66-8c7d-9bec5d6d2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_MI, high_correlation_columns_rural_MI, high_correlation_columns_urban_MI = main(df_d, df_h, county_df, columns_d, columns_h, state='MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e27a41-553e-4a61-b298-571f43777ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df_NY, high_correlation_columns_rural_NY, high_correlation_columns_urban_NY = main(df_d, df_h, county_df, columns_d, columns_h, state='NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83961a26-e9b7-40f8-b6e2-421515446ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df_WA, high_correlation_columns_rural_WA, high_correlation_columns_urban_WA = main(df_d, df_h, county_df, columns_d, columns_h, state='WA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb47e25-46b0-4c68-8bf4-ad1a90491320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df_TX, high_correlation_columns_rural_TX, high_correlation_columns_urban_TX = main(df_d, df_h, county_df, columns_d, columns_h, state='TX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db20e00-0fa8-4ee1-a108-8b572e4a045c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df_FL, high_correlation_columns_rural_FL, high_correlation_columns_urban_FL = main(df_d, df_h, county_df, columns_d, columns_h, state='FL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2bda5-9af2-4851-8bca-ab298ce7ec97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
